{"cells":[{"cell_type":"markdown","metadata":{"id":"b2nY9Hx0FsjU"},"source":["# 0) Setup (Install & Imports)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18837,"status":"ok","timestamp":1757076942503,"user":{"displayName":"Arvin Jayanake","userId":"09599054493066328195"},"user_tz":-330},"id":"9nCxmHeBFfuq","outputId":"bb371016-a77f-4109-f391-7d5c096b9346"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Collecting rapidfuzz\n","  Downloading rapidfuzz-3.14.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading rapidfuzz-3.14.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: rapidfuzz\n","Successfully installed rapidfuzz-3.14.0\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n"]}],"source":["!pip install torch torchvision torchaudio rapidfuzz\n","!pip install pillow\n","\n","import shutil, glob, os, math, random, string\n","from pathlib import Path\n","import numpy as np\n","import cv2\n","from PIL import Image, ImageDraw, ImageFont\n","\n","import os\n","import cv2\n","import math\n","import time\n","import glob\n","import random\n","import string\n","import numpy as np\n","from pathlib import Path\n","from typing import List, Tuple\n","import shutil\n","import string\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":136,"status":"ok","timestamp":1757076942755,"user":{"displayName":"Arvin Jayanake","userId":"09599054493066328195"},"user_tz":-330},"id":"DGVurKyVGxyf","outputId":"2f8344b1-13fb-4343-8ac8-529a3e08d7ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}],"source":["SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Device:\", DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"BsFkQEZkG7Hv"},"source":["# 1) Character Set & Utilities"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1757076942766,"user":{"displayName":"Arvin Jayanake","userId":"09599054493066328195"},"user_tz":-330},"id":"TYsND6CJG70_"},"outputs":[],"source":["# EXACTLY match what your generator emits\n","CHARSET = (\n","    list(string.digits) +\n","    list(string.ascii_uppercase) +\n","    list(string.ascii_lowercase) +\n","    list(\"-/:.,()&%+ #\")\n",")\n","BLANK_IDX = 0\n","IDX2CHAR = {i+1: ch for i, ch in enumerate(CHARSET)}  # 1..N => char\n","CHAR2IDX = {ch: i+1 for i, ch in enumerate(CHARSET)}  # char => 1..N\n","NUM_CLASSES = len(CHARSET) + 1  # +1 for CTC blank at index 0\n","\n","def text_to_labels(s: str):\n","    return [CHAR2IDX[ch] for ch in s if ch in CHAR2IDX]\n","\n","def labels_to_text(labels):\n","    out, prev = [], None\n","    for l in labels:\n","        if l != BLANK_IDX and l != prev:\n","            out.append(IDX2CHAR.get(l, \"\"))\n","        prev = l\n","    return \"\".join(out)"]},{"cell_type":"markdown","metadata":{"id":"6wjYFLVoHB43"},"source":["# 2) Tiny Synthetic Dataset (OpenCV-generated)"]},{"cell_type":"markdown","metadata":{"id":"FF6q0mgiQVPK"},"source":["## Random Text Gen Function"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1757076953808,"user":{"displayName":"Arvin Jayanake","userId":"09599054493066328195"},"user_tz":-330},"id":"JMzO5TJvQbZn"},"outputs":[],"source":["def _rand_text(min_len=5, max_len=12):\n","    \"\"\"\n","    Generate realistic OCR strings with weighted patterns.\n","    Ensures final length is between [min_len, max_len].\n","    \"\"\"\n","    def gen_12_digit():\n","        return \"\".join(random.choices(string.digits, k=12))\n","\n","    def gen_lienceNo():\n","      return random.choice([\"B\", \"v\"]).join(random.choices(string.digits, k=7))\n","\n","    def gen_9_digit_V():\n","        # 9 digits + space + 'V' or 'v'  (length 11)\n","        return \"\".join(random.choices(string.digits, k=9)) + \" \" + random.choice([\"V\", \"v\"])\n","\n","    def gen_title_word():\n","        # One or two simple English-looking words (Title Case)\n","        vowels = \"aeiou\"\n","        consonants = \"\".join([c for c in string.ascii_lowercase if c not in vowels])\n","        def make_syllable():\n","            # simple CV or CVC to look word-like\n","            syl = random.choice(consonants) + random.choice(vowels)\n","            if random.random() < 0.5:\n","                syl += random.choice(consonants)\n","            return syl\n","        def make_word(min_w=3, max_w=9):\n","            wlen = random.randint(min_w, max_w)\n","            # stitch 2–4 syllables to reach length target\n","            s = \"\"\n","            while len(s) < wlen:\n","                s += make_syllable()\n","            s = s[:wlen]\n","            return s.capitalize()\n","        if random.random() < 0.5:\n","            return make_word()\n","        else:\n","            return f\"{make_word()} {make_word()}\"\n","\n","    def gen_upper_code():\n","        # AB-1234 or ABC-12 etc.\n","        letters = \"\".join(random.choices(string.ascii_uppercase, k=random.randint(2, 3)))\n","        digits = \"\".join(random.choices(string.digits, k=random.randint(2, 4)))\n","        return f\"{letters}-{digits}\"\n","\n","    def gen_invoice():\n","        # INV-123456 (8–11 chars typical)\n","        return \"INV-\" + \"\".join(random.choices(string.digits, k=random.randint(4, 7)))\n","\n","    def gen_date():\n","        # DD/MM/YYYY\n","        d = random.randint(1, 28)\n","        m = random.randint(1, 12)\n","        y = random.randint(2000, 2029)\n","        return f\"{d:02d}/{m:02d}/{y:04d}\"\n","\n","    def gen_plate():\n","        # ABC-1234 (common style)\n","        letters = \"\".join(random.choices(string.ascii_uppercase, k=3))\n","        digits = \"\".join(random.choices(string.digits, k=4))\n","        return f\"{letters}-{digits}\"\n","\n","    # Weighted pattern list (func, weight)\n","    patterns = [\n","        (gen_12_digit, 3),\n","        (gen_9_digit_V, 3),\n","        (gen_lienceNo, 3),\n","        (gen_title_word, 2),\n","        (gen_upper_code, 2),\n","        (gen_invoice, 1),\n","        (gen_date, 1),\n","        (gen_plate, 1),\n","    ]\n","\n","    funcs, weights = zip(*patterns)\n","\n","    # Try up to a few times to honor length; then fall back to mixed\n","    for _ in range(6):\n","        txt = random.choices(funcs, weights=weights, k=1)[0]()\n","        if min_len <= len(txt) <= max_len:\n","            return txt\n","\n","    # fallback (guaranteed length)\n","    return gen_mixed()"]},{"cell_type":"markdown","metadata":{"id":"vZfD1o0eTXZ2"},"source":["## Apply Backgrounds"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1757076956561,"user":{"displayName":"Arvin Jayanake","userId":"09599054493066328195"},"user_tz":-330},"id":"s_pgjiMzTZfB"},"outputs":[],"source":["BACKGROUND_DIR = Path(\"./backgrounds\")\n","\n","def _list_bg_images(bg_dir: Path):\n","    exts = (\"*.png\", \"*.jpg\", \"*.jpeg\", \"*.webp\")\n","    imgs = []\n","    if bg_dir.exists():\n","        for ext in exts:\n","            imgs += sorted(glob.glob(str(bg_dir / ext)))\n","    return imgs\n","\n","def _prepare_bg(bg_bgr: np.ndarray, H: int, W: int) -> np.ndarray:\n","    \"\"\"\n","    Accepts a small cropped swatch (BGR). Tiles if smaller than target,\n","    then random-crops to exactly HxW. Applies light jitter for realism.\n","    Returns BGR.\n","    \"\"\"\n","    h, w = bg_bgr.shape[:2]\n","    # Tile if swatch is smaller than needed\n","    if h < H or w < W:\n","        rep_y = (H + h - 1) // h\n","        rep_x = (W + w - 1) // w\n","        bg_bgr = np.tile(bg_bgr, (rep_y, rep_x, 1))\n","        h, w = bg_bgr.shape[:2]\n","\n","    # Random crop to HxW\n","    y0 = random.randint(0, h - H)\n","    x0 = random.randint(0, w - W)\n","    bg_bgr = bg_bgr[y0:y0 + H, x0:x0 + W]\n","\n","    # Light jitter (optional)\n","    if random.random() < 0.5:\n","        bg_bgr = cv2.GaussianBlur(bg_bgr, (3, 3), 0)\n","    if random.random() < 0.8:\n","        alpha = random.uniform(0.85, 1.15)  # contrast-ish\n","        beta  = random.randint(-12, 12)     # brightness-ish\n","        bg_bgr = np.clip(bg_bgr.astype(np.float32) * alpha + beta, 0, 255).astype(np.uint8)\n","\n","    return bg_bgr\n"]},{"cell_type":"markdown","metadata":{"id":"JFjyxhXaVXob"},"source":["## Generate Data"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":79316,"status":"ok","timestamp":1757077225872,"user":{"displayName":"Arvin Jayanake","userId":"09599054493066328195"},"user_tz":-330},"id":"NaRXN1yBNkY-","outputId":"bec1a2ef-d107-4551-d160-f02295650d80"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fonts: 3 | Backgrounds: 11\n","Generating synthetic dataset...\n","Saved 10000 samples to synthetic_ocr/images and labels to synthetic_ocr/labels.txt\n"]}],"source":["# ----------------- Paths & config -----------------\n","SYNTH_ROOT = Path(\"./synthetic_ocr\")\n","SYNTH_IMG_DIR = SYNTH_ROOT / \"images\"\n","SYNTH_LABELS_PATH = SYNTH_ROOT / \"labels.txt\"\n","FONTS_DIR = Path(\"./fonts\")\n","BACKGROUND_DIR = Path(\"./backgrounds\")\n","\n","N_SAMPLES = 10000\n","MIN_LENGTH = 2\n","MAX_LENGTH = 20\n","\n","POOL = string.ascii_uppercase + string.ascii_lowercase + string.digits + \" -/:.,()&%+ #\"\n","\n","# ----------------- helpers -----------------\n","def _list_fonts(fonts_dir: Path):\n","    if not fonts_dir.exists(): return []\n","    return sorted(glob.glob(str(fonts_dir / \"*.ttf\"))) + sorted(glob.glob(str(fonts_dir / \"*.otf\")))\n","\n","def _list_bg_images(bg_dir: Path):\n","    if not bg_dir.exists(): return []\n","    exts = (\"*.png\",\"*.jpg\",\"*.jpeg\",\"*.webp\",\"*.bmp\")\n","    paths = []\n","    for e in exts: paths += glob.glob(str(bg_dir / e))\n","    return sorted(paths)\n","\n","def _rand_text(min_len=5, max_len=12):\n","    length = random.randint(min_len, max_len)\n","    txt = \"\".join(random.choice(POOL) for _ in range(length)).strip()\n","    return txt if txt else \"A1\"\n","\n","def _render_with_pillow(text: str, H: int = 48, W: int = 300, font_path: str = None):\n","    W = max(W, min(340, 12 * len(text) + 40))\n","    if font_path:\n","        best_size = 24\n","        for sz in [26, 28, 30, 32]:\n","            try:\n","                f = ImageFont.truetype(font_path, sz)\n","                _, ascent = f.getmetrics()\n","                if ascent <= int(0.8 * H): best_size = sz\n","            except Exception:\n","                pass\n","        font = ImageFont.truetype(font_path, best_size)\n","    else:\n","        font = ImageFont.load_default()\n","\n","    img = Image.new(\"L\", (W, H), color=255)\n","    draw = ImageDraw.Draw(img)\n","    bbox = draw.textbbox((0, 0), text, font=font)\n","    tw, th = bbox[2] - bbox[0], bbox[3] - bbox[1]\n","    x = 5\n","    y = (H - th) // 2\n","    draw.text((x, y), text, font=font, fill=30)\n","    return np.array(img, dtype=np.uint8)\n","\n","def _prepare_bg(bg_bgr, H, W):\n","    h, w = bg_bgr.shape[:2]\n","    if h < H or w < W:\n","        reps_y = math.ceil(H / h) + 1\n","        reps_x = math.ceil(W / w) + 1\n","        tiled = np.tile(bg_bgr, (reps_y, reps_x, 1))\n","        bg_bgr = tiled\n","        h, w = bg_bgr.shape[:2]\n","    y0 = random.randint(0, h - H)\n","    x0 = random.randint(0, w - W)\n","    crop = bg_bgr[y0:y0+H, x0:x0+W].copy()\n","    return cv2.GaussianBlur(crop, (3,3), 0)\n","\n","# ----------------- main -----------------\n","def make_synthetic_dataset(n_samples=N_SAMPLES, min_len=MIN_LENGTH, max_len=MAX_LENGTH):\n","    # 1) Clean & recreate root dirs (this was missing)\n","    if SYNTH_ROOT.exists():\n","        shutil.rmtree(SYNTH_ROOT, ignore_errors=True)\n","    SYNTH_ROOT.mkdir(parents=True, exist_ok=True)\n","    SYNTH_IMG_DIR.mkdir(parents=True, exist_ok=True)   # ensures labels' parent exists too\n","\n","    # 2) Collect resources\n","    font_paths = _list_fonts(FONTS_DIR)\n","    bg_paths = _list_bg_images(BACKGROUND_DIR)\n","    print(f\"Fonts: {len(font_paths)} | Backgrounds: {len(bg_paths)}\")\n","    if not font_paths:\n","        print(\"No TTF/OTF fonts found → using OpenCV Hershey fallback.\")\n","    if not bg_paths:\n","        print(\"No backgrounds found → plain white backgrounds only.\")\n","\n","    # 3) Generate\n","    lines = []\n","    print(\"Generating synthetic dataset...\")\n","    for i in range(n_samples):\n","        text = _rand_text(min_len, max_len)\n","\n","        # base text image (grayscale)\n","        if font_paths:\n","            img = _render_with_pillow(text, H=48, W=300, font_path=random.choice(font_paths))\n","        else:\n","            img = np.ones((48, 300), dtype=np.uint8) * 255\n","            font = random.choice([\n","                cv2.FONT_HERSHEY_SIMPLEX, cv2.FONT_HERSHEY_DUPLEX,\n","                cv2.FONT_HERSHEY_PLAIN, cv2.FONT_HERSHEY_COMPLEX_SMALL\n","            ])\n","            font_scale = random.uniform(0.8, 1.2)\n","            thickness = random.randint(1, 2)\n","            (tw, th), _ = cv2.getTextSize(text, font, font_scale, thickness)\n","            x = 5\n","            y = (img.shape[0] + th) // 2 - 3\n","            cv2.putText(img, text, (x, y), font, font_scale, (0,), thickness, cv2.LINE_AA)\n","\n","        # rotation\n","        ang = random.uniform(-4, 4)\n","        M = cv2.getRotationMatrix2D((img.shape[1]//2, img.shape[0]//2), ang, 1.0)\n","        img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]), borderValue=255)\n","\n","        # perspective\n","        if random.random() < 0.3:\n","            h, w = img.shape[:2]\n","            src = np.float32([[0,0],[w-1,0],[0,h-1],[w-1,h-1]])\n","            dx = random.randint(0, 6)\n","            dy = random.randint(0, 4)\n","            dst = np.float32([[0+dx,0+dy],[w-1-dx,0],[0,h-1],[w-1-dx,h-1-dy]])\n","            P = cv2.getPerspectiveTransform(src, dst)\n","            img = cv2.warpPerspective(img, P, (w, h), borderValue=255)\n","\n","        # optional textured background\n","        if bg_paths:\n","            bg = cv2.imread(random.choice(bg_paths), cv2.IMREAD_COLOR)\n","            if bg is not None:\n","                H, W = img.shape[:2]\n","                bg = _prepare_bg(bg, H, W)\n","                alpha = (255.0 - img.astype(np.float32)) / 255.0\n","                out = bg.astype(np.float32) * (1.0 - alpha[..., None])\n","                img = np.clip(out, 0, 255).astype(np.uint8)  # now BGR\n","\n","        # light noise\n","        if random.random() < 0.5:\n","            noise = np.random.normal(0, 5, img.shape).astype(np.int16)\n","            img = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n","\n","        # save\n","        fname = f\"img_{i:05d}.png\"\n","        cv2.imwrite(str(SYNTH_IMG_DIR / fname), img)\n","        lines.append(f\"{fname}\\t{text}\")\n","\n","    # 4) Write labels (parent exists now)\n","    with open(SYNTH_LABELS_PATH, \"w\", encoding=\"utf-8\") as f:\n","        f.write(\"\\n\".join(lines))\n","\n","    print(f\"Saved {len(lines)} samples to {SYNTH_IMG_DIR} and labels to {SYNTH_LABELS_PATH}\")\n","\n","# run\n","make_synthetic_dataset()"]},{"cell_type":"markdown","metadata":{"id":"v857ja8Up0kS"},"source":["# 3) Data Loading and Preparation"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1757076973396,"user":{"displayName":"Arvin Jayanake","userId":"09599054493066328195"},"user_tz":-330},"id":"1aq9AuVcrVXx"},"outputs":[],"source":["def collate_fn(batch):\n","    images, targets, target_lengths = zip(*batch)\n","\n","    # Stack images (they are already the same size)\n","    images = torch.stack(images, 0)\n","\n","    # Find max target length in this batch\n","    max_target_len = max(target_lengths).item()\n","\n","    # Pad all targets to max length\n","    padded_targets = []\n","    for target in targets:\n","        pad_size = max_target_len - len(target)\n","        padded_target = torch.cat([target, torch.zeros(pad_size, dtype=torch.long)])\n","        padded_targets.append(padded_target)\n","\n","    targets = torch.stack(padded_targets, 0)\n","    target_lengths = torch.stack(target_lengths, 0)\n","\n","    return images, targets, target_lengths"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1757076973460,"user":{"displayName":"Arvin Jayanake","userId":"09599054493066328195"},"user_tz":-330},"id":"IVCiLf-0p1Fo","outputId":"2f4a8211-eb03-4743-9b93-32095d3284fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 10000 samples\n"]}],"source":["class OCRDataset(Dataset):\n","    def __init__(self, labels_file, img_dir, img_height=32, img_width=128):\n","        self.img_dir = Path(img_dir)\n","        self.img_height = img_height\n","        self.img_width = img_width\n","\n","        # Read labels\n","        with open(labels_file, 'r', encoding='utf-8') as f:\n","            lines = f.readlines()\n","\n","        self.samples = []\n","        for line in lines:\n","            parts = line.strip().split('\\t')\n","            if len(parts) >= 2:\n","                self.samples.append((parts[0], parts[1]))\n","\n","        print(f\"Loaded {len(self.samples)} samples\")\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        img_name, text = self.samples[idx]\n","        img_path = self.img_dir / img_name\n","\n","        # Load and preprocess image\n","        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n","        if img is None:\n","            # Fallback to a blank image if loading fails\n","            img = np.ones((self.img_height, self.img_width), dtype=np.uint8) * 255\n","\n","        # Resize with aspect ratio preservation\n","        h, w = img.shape\n","        new_w = int(w * (self.img_height / h))\n","        img = cv2.resize(img, (new_w, self.img_height), interpolation=cv2.INTER_AREA)\n","\n","        # Pad to fixed width\n","        if new_w < self.img_width:\n","            pad_width = self.img_width - new_w\n","            img = np.pad(img, ((0, 0), (0, pad_width)), mode='constant', constant_values=255)\n","        else:\n","            img = img[:, :self.img_width]\n","\n","        # Normalize and convert to tensor\n","        img = img.astype(np.float32) / 255.0\n","        img = (img - 0.5) / 0.5  # Normalize to [-1, 1]\n","        img_tensor = torch.from_numpy(img).unsqueeze(0)  # Add channel dimension\n","\n","        # Convert text to label indices\n","        target = text_to_labels(text)\n","        target_length = torch.tensor([len(target)], dtype=torch.long)\n","        target = torch.tensor(target, dtype=torch.long)\n","\n","        return img_tensor, target, target_length\n","\n","# Create datasets\n","train_dataset = OCRDataset(SYNTH_LABELS_PATH, SYNTH_IMG_DIR)\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=32,\n","    shuffle=True,\n","    num_workers=2,\n","    collate_fn=collate_fn\n",")"]},{"cell_type":"markdown","metadata":{"id":"WA5JQQK9p9VV"},"source":["# 4) CRNN Model Definition"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iYV0q1dJp94L","executionInfo":{"status":"ok","timestamp":1757076976548,"user_tz":-330,"elapsed":272,"user":{"displayName":"Arvin Jayanake","userId":"09599054493066328195"}},"outputId":"2a9632a5-8c68-4140-c07b-08add1a66531"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model has 7,693,643 parameters\n"]}],"source":["class CRNN(nn.Module):\n","    def __init__(self, img_channel, img_height, img_width, num_classes, hidden_size=256):\n","        super(CRNN, self).__init__()\n","\n","        # CNN layers\n","        self.cnn = nn.Sequential(\n","            nn.Conv2d(img_channel, 64, 3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),\n","            nn.Conv2d(64, 128, 3, stride=1, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),\n","            nn.Conv2d(128, 256, 3, stride=1, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n","            nn.Conv2d(256, 256, 3, stride=1, padding=1), nn.ReLU(),\n","            nn.Conv2d(256, 512, 3, stride=1, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n","            nn.Conv2d(512, 512, 3, stride=1, padding=1), nn.ReLU(),\n","            nn.AdaptiveAvgPool2d((1, None))  # Adaptive pooling to handle variable width\n","        )\n","\n","        # Calculate LSTM input features\n","        with torch.no_grad():\n","            dummy_input = torch.zeros(1, img_channel, img_height, img_width)\n","            dummy_output = self.cnn(dummy_input)\n","            lstm_input_size = dummy_output.size(1) * dummy_output.size(2)\n","\n","        # RNN layers (LSTM)\n","        self.lstm = nn.LSTM(\n","            input_size=lstm_input_size,\n","            hidden_size=hidden_size,\n","            num_layers=2,\n","            bidirectional=True,\n","            batch_first=True\n","        )\n","\n","        # Output layer\n","        self.fc = nn.Linear(hidden_size * 2, num_classes)  # *2 for bidirectional\n","\n","    def forward(self, x):\n","        # CNN\n","        x = self.cnn(x)\n","\n","        # Prepare for LSTM\n","        batch, channels, height, width = x.size()\n","        x = x.permute(0, 3, 1, 2)  # [batch, width, channels, height]\n","        x = x.reshape(batch, width, channels * height)  # [batch, width, channels*height]\n","\n","        # LSTM\n","        x, _ = self.lstm(x)\n","\n","        # Output layer\n","        x = self.fc(x)\n","        x = x.permute(1, 0, 2)  # [width, batch, num_classes] for CTC\n","\n","        return x\n","\n","# Initialize model\n","model = CRNN(\n","    img_channel=1,\n","    img_height=32,\n","    img_width=128,\n","    num_classes=NUM_CLASSES\n",").to(DEVICE)\n","\n","print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")"]},{"cell_type":"markdown","metadata":{"id":"GSvBZLfGqFTj"},"source":["# 5) Training Setup"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"83a0GwkFqF0A","executionInfo":{"status":"ok","timestamp":1757076986215,"user_tz":-330,"elapsed":5084,"user":{"displayName":"Arvin Jayanake","userId":"09599054493066328195"}}},"outputs":[],"source":["# Loss function (CTC)\n","criterion = nn.CTCLoss(blank=BLANK_IDX, zero_infinity=True)\n","\n","# Optimizer\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"]},{"cell_type":"markdown","metadata":{"id":"1xCqGkAFqfjP"},"source":["# 6) Training Loop"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"3BiQVxLQqiv2","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1757077046029,"user_tz":-330,"elapsed":59705,"user":{"displayName":"Arvin Jayanake","userId":"09599054493066328195"}},"outputId":"5259da9d-f937-40cc-8603-23013a977b11"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting training...\n","Epoch: 1/30 | Batch: 0/313 | Loss: -2.9227\n","Epoch: 1/30 | Batch: 20/313 | Loss: 7.0321\n","Epoch: 1/30 | Batch: 40/313 | Loss: 5.4155\n","Epoch: 1/30 | Batch: 60/313 | Loss: 5.9717\n","Epoch: 1/30 | Batch: 80/313 | Loss: 5.3376\n","Epoch: 1/30 | Batch: 100/313 | Loss: 4.9195\n","Epoch: 1/30 | Batch: 120/313 | Loss: 5.0653\n","Epoch: 1/30 | Batch: 140/313 | Loss: 4.6093\n","Epoch: 1/30 | Batch: 160/313 | Loss: 5.0029\n","Epoch: 1/30 | Batch: 180/313 | Loss: 4.8764\n","Epoch: 1/30 | Batch: 200/313 | Loss: 5.2081\n","Epoch: 1/30 | Batch: 220/313 | Loss: 4.9635\n","Epoch: 1/30 | Batch: 240/313 | Loss: 4.6636\n","Epoch: 1/30 | Batch: 260/313 | Loss: 4.6830\n","Epoch: 1/30 | Batch: 280/313 | Loss: 5.1866\n","Epoch: 1/30 | Batch: 300/313 | Loss: 4.8467\n","Epoch 1 completed | Avg Loss: 4.9715 | Accuracy: 0.00%\n","Epoch: 2/30 | Batch: 0/313 | Loss: 4.8250\n","Epoch: 2/30 | Batch: 20/313 | Loss: 4.9096\n","Epoch: 2/30 | Batch: 40/313 | Loss: 4.6700\n","Epoch: 2/30 | Batch: 60/313 | Loss: 4.5871\n","Epoch: 2/30 | Batch: 80/313 | Loss: 5.1533\n","Epoch: 2/30 | Batch: 100/313 | Loss: 4.9446\n","Epoch: 2/30 | Batch: 120/313 | Loss: 4.7163\n","Epoch: 2/30 | Batch: 140/313 | Loss: 4.5135\n","Epoch: 2/30 | Batch: 160/313 | Loss: 4.8422\n","Epoch: 2/30 | Batch: 180/313 | Loss: 4.8264\n","Epoch: 2/30 | Batch: 200/313 | Loss: 4.9252\n","Epoch: 2/30 | Batch: 220/313 | Loss: 5.0536\n","Epoch: 2/30 | Batch: 240/313 | Loss: 4.7928\n","Epoch: 2/30 | Batch: 260/313 | Loss: 4.7559\n","Epoch: 2/30 | Batch: 280/313 | Loss: 4.7145\n","Epoch: 2/30 | Batch: 300/313 | Loss: 4.6672\n","Epoch 2 completed | Avg Loss: 4.8295 | Accuracy: 0.00%\n","Epoch: 3/30 | Batch: 0/313 | Loss: 4.9630\n","Epoch: 3/30 | Batch: 20/313 | Loss: 4.7276\n","Epoch: 3/30 | Batch: 40/313 | Loss: 4.6780\n","Epoch: 3/30 | Batch: 60/313 | Loss: 4.7545\n","Epoch: 3/30 | Batch: 80/313 | Loss: 4.7004\n","Epoch: 3/30 | Batch: 100/313 | Loss: 4.5602\n","Epoch: 3/30 | Batch: 120/313 | Loss: 4.6166\n","Epoch: 3/30 | Batch: 140/313 | Loss: 4.6156\n","Epoch: 3/30 | Batch: 160/313 | Loss: 4.8826\n","Epoch: 3/30 | Batch: 180/313 | Loss: 4.6701\n","Epoch: 3/30 | Batch: 200/313 | Loss: 4.5068\n","Epoch: 3/30 | Batch: 220/313 | Loss: 4.7597\n","Epoch: 3/30 | Batch: 240/313 | Loss: 4.5707\n","Epoch: 3/30 | Batch: 260/313 | Loss: 4.6638\n","Epoch: 3/30 | Batch: 280/313 | Loss: 4.5593\n","Epoch: 3/30 | Batch: 300/313 | Loss: 4.6130\n","Epoch 3 completed | Avg Loss: 4.6696 | Accuracy: 0.00%\n","Epoch: 4/30 | Batch: 0/313 | Loss: 4.6363\n","Epoch: 4/30 | Batch: 20/313 | Loss: 4.5852\n","Epoch: 4/30 | Batch: 40/313 | Loss: 4.4954\n","Epoch: 4/30 | Batch: 60/313 | Loss: 4.6321\n","Epoch: 4/30 | Batch: 80/313 | Loss: 4.7247\n","Epoch: 4/30 | Batch: 100/313 | Loss: 4.6155\n","Epoch: 4/30 | Batch: 120/313 | Loss: 4.4369\n","Epoch: 4/30 | Batch: 140/313 | Loss: 4.5976\n","Epoch: 4/30 | Batch: 160/313 | Loss: 4.4526\n","Epoch: 4/30 | Batch: 180/313 | Loss: 4.6641\n","Epoch: 4/30 | Batch: 200/313 | Loss: 4.5140\n","Epoch: 4/30 | Batch: 220/313 | Loss: 4.5849\n","Epoch: 4/30 | Batch: 240/313 | Loss: 4.5901\n","Epoch: 4/30 | Batch: 260/313 | Loss: 4.5807\n","Epoch: 4/30 | Batch: 280/313 | Loss: 4.4769\n","Epoch: 4/30 | Batch: 300/313 | Loss: 4.5269\n","Epoch 4 completed | Avg Loss: 4.5781 | Accuracy: 0.00%\n","Epoch: 5/30 | Batch: 0/313 | Loss: 4.6082\n","Epoch: 5/30 | Batch: 20/313 | Loss: 4.7097\n","Epoch: 5/30 | Batch: 40/313 | Loss: 4.6400\n","Epoch: 5/30 | Batch: 60/313 | Loss: 4.5946\n","Epoch: 5/30 | Batch: 80/313 | Loss: 4.5570\n","Epoch: 5/30 | Batch: 100/313 | Loss: 4.2303\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2807497239.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-2807497239.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mtarget_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["def train_model(model, dataloader, criterion, optimizer, num_epochs=25):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0\n","        correct = 0\n","        total = 0\n","\n","        for batch_idx, (data, targets, target_lengths) in enumerate(dataloader):\n","            data = data.to(DEVICE)\n","            targets = targets.to(DEVICE)\n","            target_lengths = target_lengths.to(DEVICE)\n","\n","            # Forward pass\n","            optimizer.zero_grad()\n","            outputs = model(data)\n","\n","            # Prepare for CTC loss\n","            input_lengths = torch.full(\n","                size=(outputs.size(1),),\n","                fill_value=outputs.size(0),\n","                dtype=torch.long\n","            ).to(DEVICE)\n","\n","            # Calculate loss\n","            loss = criterion(outputs, targets, input_lengths, target_lengths)\n","\n","            # Backward pass\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Statistics\n","            epoch_loss += loss.item()\n","\n","            # Calculate accuracy (greedy decoding)\n","            _, max_probs = torch.max(outputs.permute(1, 0, 2), 2)\n","            predictions = []\n","            for i in range(max_probs.size(0)):\n","                raw_pred = max_probs[i].cpu().numpy()\n","                pred_text = labels_to_text(raw_pred)\n","                predictions.append(pred_text)\n","\n","            # Compare with ground truth\n","            for i, (pred, target_len) in enumerate(zip(predictions, target_lengths)):\n","                # Only use the non-padded part of the target\n","                target_text = labels_to_text(targets[i][:target_len].cpu().numpy())\n","                if pred == target_text:\n","                    correct += 1\n","                total += 1\n","\n","            if batch_idx % 20 == 0:\n","                print(f'Epoch: {epoch+1}/{num_epochs} | '\n","                      f'Batch: {batch_idx}/{len(dataloader)} | '\n","                      f'Loss: {loss.item():.4f}')\n","\n","        scheduler.step()\n","        accuracy = 100 * correct / total if total > 0 else 0\n","        avg_loss = epoch_loss / len(dataloader)\n","        print(f'Epoch {epoch+1} completed | '\n","              f'Avg Loss: {avg_loss:.4f} | '\n","              f'Accuracy: {accuracy:.2f}%')\n","\n","    return model\n","\n","# Train the model\n","print(\"Starting training...\")\n","model = train_model(model, train_loader, criterion, optimizer, num_epochs=30)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SXPtrhprxb0x"},"outputs":[],"source":["\"\"\"# 8) Model Evaluation and Export\"\"\"\n","\n","def predict_image(model, image_path):\n","    # Load and preprocess image\n","    img = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n","    if img is None:\n","        return \"Error: Could not load image\"\n","\n","    # Preprocess like in dataset\n","    h, w = img.shape\n","    new_w = int(w * (32 / h))\n","    img = cv2.resize(img, (new_w, 32), interpolation=cv2.INTER_AREA)\n","\n","    if new_w < 128:\n","        pad_width = 128 - new_w\n","        img = np.pad(img, ((0, 0), (0, pad_width)), mode='constant', constant_values=255)\n","    else:\n","        img = img[:, :128]\n","\n","    # Normalize and convert to tensor\n","    img = img.astype(np.float32) / 255.0\n","    img = (img - 0.5) / 0.5\n","    img_tensor = torch.from_numpy(img).unsqueeze(0).unsqueeze(0).to(DEVICE)  # [1, 1, H, W]\n","\n","    # Inference\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(img_tensor)\n","        _, max_probs = torch.max(outputs.permute(1, 0, 2), 2)\n","        raw_pred = max_probs[0].cpu().numpy()\n","        pred_text = labels_to_text(raw_pred)\n","\n","    return pred_text\n","\n","# Test on a few samples\n","test_samples = random.sample(train_dataset.samples, min(5, len(train_dataset.samples)))\n","print(\"\\nTesting on sample images:\")\n","for img_name, true_text in test_samples:\n","    img_path = SYNTH_IMG_DIR / img_name\n","    pred_text = predict_image(model, img_path)\n","    print(f\"Image: {img_name} | True: '{true_text}' | Pred: '{pred_text}'\")\n","\n","# Save the trained model\n","def save_model(model, path):\n","    torch.save({\n","        'model_state_dict': model.state_dict(),\n","        'charset': CHARSET,\n","        'img_height': 32,\n","        'img_width': 128\n","    }, path)\n","    print(f\"Model saved to {path}\")\n","\n","MODEL_PATH = \"./crnn_ocr_model.pth\"\n","save_model(model, MODEL_PATH)\n","\n","print(\"Training and export completed successfully!\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyOrQcUE1QBU7tIhnGrkFQl0"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}